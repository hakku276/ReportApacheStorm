% This template has been tested with LLNCS DOCUMENT CLASS -- version 2.20 (24-JUN-2015)

%"runningheads" enables:
%  - page number on page 2 onwards
%  - title/authors on even/odd pages
%This is good for other readers to enable proper archiving among other papers and pointing to
%content. Even if the title page states the title, when printed and stored in a folder, when
%blindly opening the folder, one could hit not the title page, but an arbitrary page. Therefore,
%it is good to have title printed on the pages, too.
\documentclass[runningheads,a4paper]{llncs}[2015/06/24]

%cmap has to be loaded before any font package (such as cfr-lm)
\usepackage{cmap}
\usepackage[T1]{fontenc}

\usepackage{graphicx}

% für neue deutsche Rechtschreibung
%\usepackage[english,ngerman]{babel}

% für englische Rechtschreibung
%Even though `american`, `english` and `USenglish` are synonyms for babel package (according to https://tex.stackexchange.com/questions/12775/babel-english-american-usenglish), the llncs document class is prepared to avoid the overriding of certain names (such as "Abstract." -> "Abstract" or "Fig." -> "Figure") when using `english`, but not when using the other 2.
%english has to go last to set it as default language
\usepackage[ngerman,english]{babel}

%Eingabeformat UTF-8
\usepackage[utf8]{inputenc}

%Hint by http://tex.stackexchange.com/a/321066/9075 -> enable "= as dashes
\addto\extrasenglish{\languageshorthands{ngerman}\useshorthands{"}}

%cfr-lm is preferred over lmodern. Reasoning at http://tex.stackexchange.com/a/247543/9075
\usepackage[%
rm={oldstyle=false,proportional=true},%
sf={oldstyle=false,proportional=true},%
tt={oldstyle=false,proportional=true,variable=true},%
qt=false%
]{cfr-lm}
%
%if more space is needed, exchange cfr-lm by mathptmx

\graphicspath{{graphics/}}

%Tweaks by IPVS/AS
\usepackage{lncs_as}

%for demonstration purposes only
\usepackage[math]{blindtext}

%Sorts the citations in the brackets
%It also allows \cite{refa, refb}. Otherwise, the document does not compile.
%  Error message: "White space in argument"
\usepackage{cite}


%% If you need packages for other papers,
%% START COPYING HERE
%% COPY ALSO cmap and fontenc from lines 10 to 12

%extended enumerate, such as \begin{compactenum}
\usepackage{paralist}

%put figures inside a text
%\usepackage{picins}
%use
%\piccaptioninside
%\piccaption{...}
%\parpic[r]{\includegraphics ...}
%Text...

%for easy quotations: \enquote{text}
\usepackage{csquotes}

%enable margin kerning
\usepackage{microtype}

%tweak \url{...}
\usepackage{url}
%\urlstyle{same}
%improve wrapping of URLs - hint by http://tex.stackexchange.com/a/10419/9075
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
%nicer // - solution by http://tex.stackexchange.com/a/98470/9075
%DO NOT ACTIVATE -> prevents line breaks
%\makeatletter
%\def\Url@twoslashes{\mathchar`\/\@ifnextchar/{\kern-.2em}{}}
%\g@addto@macro\UrlSpecials{\do\/{\Url@twoslashes}}
%\makeatother

%diagonal lines in a table - http://tex.stackexchange.com/questions/17745/diagonal-lines-in-table-cell
%slashbox is not available in texlive (due to licensing) and also gives bad results. This, we use diagbox
%\usepackage{diagbox}

%required for pdfcomment later
\usepackage[hyperref,svgnames]{xcolor}

\usepackage{listings}
\lstloadlanguages{java}
\lstset{language=java,numbers=left,captionpos=b}


%enable nice comments
%this also loads hyperref
\usepackage{pdfcomment}
%enable hyperref without colors and without bookmarks
\hypersetup{hidelinks,
   colorlinks=true,
   allcolors=black,
   pdfstartview=Fit,
   breaklinks=true}
%enables correct jumping to figures when referencing
\usepackage[all]{hypcap}

\newcommand{\commentontext}[2]{\colorbox{yellow!60}{#1}\pdfcomment[color={0.234 0.867 0.211},hoffset=-6pt,voffset=10pt,opacity=0.5]{#2}}
\newcommand{\commentatside}[1]{\pdfcomment[color={0.045 0.278 0.643},icon=Note]{#1}}

%compatibality with packages todo, easy-todo, todonotes
\newcommand{\todo}[1]{\commentatside{#1}}
%compatiblity with package fixmetodonotes
\newcommand{\TODO}[1]{\commentatside{#1}}

%enable \cref{...} and \Cref{...} instead of \ref: Type of reference included in the link

%\usepackage[capitalise,nameinlink,ngerman]{cleveref}
\usepackage[capitalise,nameinlink,english]{cleveref}
%Nice formats for \cref - only for English texts
%\crefname{section}{Sect.}{Sect.}
%\Crefname{section}{Section}{Sections}

\usepackage{xspace}
%\newcommand{\eg}{e.\,g.\xspace}
%\newcommand{\ie}{i.\,e.\xspace}
\newcommand{\eg}{e.\,g.,\ }
\newcommand{\ie}{i.\,e.,\ }

%introduce \powerset - hint by http://matheplanet.com/matheplanet/nuke/html/viewtopic.php?topic=136492&post_id=997377
\DeclareFontFamily{U}{MnSymbolC}{}
\DeclareSymbolFont{MnSyC}{U}{MnSymbolC}{m}{n}
\DeclareFontShape{U}{MnSymbolC}{m}{n}{
    <-6>  MnSymbolC5
   <6-7>  MnSymbolC6
   <7-8>  MnSymbolC7
   <8-9>  MnSymbolC8
   <9-10> MnSymbolC9
  <10-12> MnSymbolC10
  <12->   MnSymbolC12%
}{}
\DeclareMathSymbol{\powerset}{\mathord}{MnSyC}{180}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%% END COPYING HERE


\begin{document}

\title{Apache Storm}
%If Title is too long, use \titlerunning
%\titlerunning{Short Title}

\author{Aanal Raj Basaula}

\supervisor{Matthias Wieland}

\seminar{Advanced Topics in Data Management}

\semester{WS 2017/2018}

\abgabedatum{Stuttgart, 11.12.2017}

\institute{\email{aanal.basaula@gmail.com}}

%\frontpagede % creates the frontpage (in German)
\frontpageen % creates the frontpage (in English)

\thispagestyle{empty}
\cleardoublepage

\maketitle

\begin{abstract}
Apache Storm is one of the available open source tools for Stream Data Processing. This document provides a brief overview of the intricacies of the Architecture of Storm and tends to proceed further on the details of considerations required to design a Solution based on Storm. The design considerations relating to deployment of Storm System as a Lambda or a Lambda-less Architecture and the sketch of the various internal components of Storm i.e. Bolts, Spouts. We consider a practical implementation of Storm to try and express the theoretical design considerations in real practice. Finally we list out the other available tools and compare them to Storm with regard to Performance, Resilience and various other parameters.
\end{abstract}

\begin{keywords}
Apache Storm, Stream Processing, Real Time Processing, Big Data
\end{keywords}

\section{Introduction}

\textbf{Data} today is generated by a lot of devices that are connected to the internet and the amount of data being generated at any instance is increasing rapidly. These devices could include a personal computer, a smart phone, or even an embedded device. Due to the rapid adoption of Internet of Things, data is being generated at an unprecedented rate. The sheer volume of data generated and the rate at which it is being generated can be termed as Big Data. \textbf{Big Data} made it's appearance in 1998 SGI slide by John Mashey and now is of interest to a lot of financial as well as research institutions.\cite{miningstatus}

Data can be considered the oil of this present age, because with huge amounts of data one can determine hidden patterns. A lot of companies utilize pattern recognition on big data to open opportunities for revenue generation whereas research institutions use it to discover patterns simply not observable by the human mind(simply due to the volume of data). The question on how to classify any data as big data can be answered by using the 3Vs: Volume, Variety and Velocity \cite{usingapachestorm}. Any data with big volume or variety (such as text, video, images, etc) or that gets generated rapidly can be considered as big data. In this paper regarding \textbf{Apache Storm} we consider the third V of big data to be the influencing factor.

\textbf{Data Stream} is a sequence of data flowing from a source to a destination, for example a sensor reporting the temperature to a server. The sensor measures the temperature periodically and updates it to the server. Another example could also be the purchase requests of products in a shopping site. In all of these scenarios, the data is continuously being generated. Traditionally analyzing of these types of data requires storage of these data onto persistence and later batch processing it to discern patterns as well as other useful information. But, with the help of \textbf{Apache Storm} we are able to process these data as they are generated with minimum amount of latency.

This benefit could be a major game changer for many companies as Stream processing of data allows Real Time Analysis. As an example, an online store could monitor its purchase requests for best performing and least performing articles and could offer discounts on the articles not performing good to increase their sales volume. This small change could increase the gross income of a company and will surely be advantageous.

\section{Apache Storm: Concepts}
\label{sec:concepts}

Apache storm is one of the available open source tools for stream data processing, which is considered to be one of the most easy to use, fast and fault tolerant systems available \cite{stormperspective}. These three factors have been the major points for the general public to choose \textbf{Apache Storm} above other available tools. It was originally created by Nathan Marz at BackType and till now, at present more than 60 companies use or experiment with this tool.

\subsection{Architecture Overview}
The basic Storm data processing architecture consists of stream of tuples flowing through a defined topology to yield the required analytic result. Processing of this stream data is performed in Nodes called the worker nodes, which are responsible for performing the required calculations. There is also another node named the Master Node, which is responsible for the complete Storm infrastructure,  by keeping track of all the worker nodes and the total topology itself.

\begin{figure}
  \begin{center}
    \includegraphics[width=.7\textwidth]{arch.png}
    \caption{High Level Storm Architecture}
    \label{fig:arch}
   \end{center}
\end{figure}

\subsubsection{Nimbus}

\paragraph{The Master node} also called the \textbf{Nimbus} is also responsible for distribution and coordination of the execution of topologies. The Nimbus is the first point where the user submits the defined topology, which is then distributed among the worker nodes.  Figure \ref{fig:arch} illustrates the high level architecture where, Nimbus monitors all the available worker nodes and both the nimbus as well as the worker nodes use \textbf{Apache Zookeeper} to store their states. The actual processing is done by the \textbf{worker nodes}, and each worker node runs multiple Worker Processes each of them mapped to their own topology \cite{stormtwitter}. It is also worth while noting that multiple worker processes in a worker node could be executing different parts of the topology.

\subsubsection{Worker Node}
\paragraph{Worker Node} is responsible for all the heavy lifting required for processing of the topology. One worker node may run a multiple number of worker processes. Each of these worker processes may belong to the same topology or different topologies. A worker node maintains a \textbf{Supervisor} and a multiple number of Worker Processes which in turn maintains a multiple number of executors. Thus, Executors enable intra-topology parallelism whereas Tasks provide intra-spout or intra-bolt parallelism. Figure \ref{fig:workerarch} depicts the design of a Worker node.\cite{stormtwitter}

\begin{figure}
  \begin{center}
    \includegraphics[width=.7\textwidth]{worker.png}
    \caption{Worker Node Architecture}
    \label{fig:workerarch}
   \end{center}
\end{figure}

\paragraph{Supervisor} is running in every Worker Node and communicates with the Nimbus to receive assignments from it. Apart from taking assignments from the Nimbus it is also responsible for monitoring the health of the of the worker processes and respawns the worker processes in case it dies.

\paragraph{Worker Process} is the Entity that does the processing and is mapped to a certain topology. Each worker node spawns a JVM and runs one or multiple executors. These executors may be running any of the bolt or spout within the topology, thus providing an added level of parallelism.

\paragraph{Executors} are made up of one or multiple Tasks. Unless specified explicitly, storm assigns single task to a single executor. Executors takes tuples from the in queue, examines the task this tuple is destined to, performs the task and then places the result in the out queue \cite{stormtwitter}.

\subsection{Topology}
\paragraph{Topology} is a directed graph where the vertices represent computation and the edges represents the data flow. Another important thing about topologies in Storm is that they are allowed to have cycles. Figure \ref{fig:topo} shows a basic Storm Topology, which consists of Spouts and Bolts. Spouts are the Sources of tuples in a Topology whereas Bolts are the units which process these generated tuples. The user first submits the topology to the Apache Storm Nimbus, which consults with each of the available Supervisors in the worker nodes and assigns each of them with tasks related to the topology.

\begin{figure}
  \begin{center}
    \includegraphics[width=.7\textwidth]{topo.png}
    \caption{Generic Storm Topology}
    \label{fig:topo}
   \end{center}
\end{figure}

\subsubsection{Spout} 
\paragraph{Spouts} are the entry point for any data in the Storm Topology. As shown in the figure \ref{fig:topo} spouts take data from external source and emits them into the topology as tuples. The input source could be a Heterogenous data source.

\subsubsection{Bolt} 
\paragraph{Bolts} are the processing entities of a Topology. It receives raw tuples emitted by spouts or processed tuples sent by another bolt and sends it down the pipe after it has completed its processing.

\section{Stream Processing}
A stream is a continuous flow of data from one point to the another. A Stream Processing system works on these data streams, the work may just be simple transformations, but also could be as complex as predictive analysis. A stream processing system depending on the nature of application has different requirements. 

Let us consider a Streaming service (may it be video or audio), which performs some transformations on the data stream before serving the client. A loss in packets is not a major problem as a little degradation in Quality for a short period of time is acceptable in this scenario. As a counter example scenario, consider a service that keeps track of the stream of purchases, it is necessary to not miss any of these important data. Apart from this, we also need to consider various other factors for design of a stream processing system, such as resilience, scalability, etc. These considerations will be scrutinized in this section.

\subsection{System Architecture}
As discussed above, a system depending upon the requirement is allowed to drop a few data sets thus allowing us to utilize the lambda-less architecture, whereas others require complete data sets therefore utilizing the Lambda Architecture.

\subsubsection{Lambda-less Architecture}
\paragraph{Apache Storm} can handle data processing without the need of persistence layer and offers its own 
resiliency mechanisms to make sure data is tuples injected into the system are processed. Section \ref{sec:resilience} explains in further detail how storm tries and guarantees a reliable throughput.

Lambda less architectures are not reliable and do not tend to provide persistence storage as every result is stored in memory and is suitable only for systems which require no storage of calculated information. In section \ref{sec:usecase}

\subsubsection{Lambda Architecture}
\paragraph{Lambda Architecture} is a data processing Architecture designed to handle high volume data utilizing both batch and stream processing. Apache storm allows users to integrate Persistence models into the system. This flexibility allows users to integrate batch processing into the system as well as seen in figure \ref{fig:lambda}.

Data is persisted in the persistence layer so that in cases of high data volume, Batch processing can be done on the input data to provide guaranteed processing and to reduce the computation requirement on the Real Time Data Processing Unit.\cite{lambdastorm}

\begin{figure}
  \begin{center}
    \includegraphics[width=.7\textwidth]{lambda.png}
    \caption{Lambda Architecture}
    \label{fig:lambda}
   \end{center}
\end{figure}

\subsection{Topology Design Considerations}
A storm topology design with the spouts and bolts should be properly considered. A spout should inject data into the topology from external sources where as the bolt should be a smallest implementable task. A storm topology should also be considered with various considerations in mind:

\subsubsection{Scalability:} A designed topology should be considered with scalability in mind. The major question is what configuration of bolts enables us to process greater amount of data at real time. Let us consider a simple example of word count topology which counts the repetition of words and ranks the most used words. It can be performed by simply including a spout and two bolts, the first of which to count the number of words and the second to aggregate and rank it (see fig. \ref{fig:ranking} Topology Setup 1).
Since we should have only one instance of Aggregator, to be able to get the aggregated result, in cases of increased data load, the number of rolling count bolts can be increased, but the Aggregation Bolt cannot be increases thus adding increased load at this node. To relax the demand on this node, we can introduce another bolt, the Intermediate Ranking Bolt, which is responsible for ranking the words at a certain predefined intervals and passes the intermediate ranks to the aggregator. This reduces load on the Aggregator since we can increase the intermediate ranking bolts to cope with the increased load.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{ranking.png}
    \caption{Possible Topologies for Word Count and Ranking}
    \label{fig:ranking}
   \end{center}
\end{figure}
\subsubsection{Resilience:} Apache storm includes APIs which allow users to develop solutions which are resilient in nature. Using the acker bolt as show in figure \ref{fig:acker}, users can track the progress of the tuple injected into the system. Thus, writing a spout which can replay the tuple in case of failures allows us to guarantee the processing of data.
\subsubsection{Interoperability:} The designed topology should be interoperable with different sources of data, and by default should work out of the box by adding simply other Spouts in the topology. This also amounts to the maintainability of the code.

\subsection{Storm Resilience}
\label{sec:resilience}

It has two types of processing schemes, \textbf{at least once} where each tuple is processed as the name suggests, at least once, and the other \textbf{at most once}, where the tuples are either processed once or dropped in cases of failure. To provide \textbf{at least once} semantics, Storm maintains an acker bolt, which keeps track of the tuple flow in the directed acyclic graph.

\begin{figure}
  \begin{center}
    \includegraphics[width=.7\textwidth]{acker.png}
    \caption{Storm Acker Bolt \cite{stormtwitter}}
    \label{fig:acker}
   \end{center}
\end{figure}

\paragraph{Figure} \ref{fig:acker} shows the Acker bolt integration with the storm topology. It keeps track of the tuple flow starting from the point where it has been injected into the topology till it leaves the topology. After the tuple has left the topology, an \textbf{ack} is sent to the Spout, at this point, it can decide to remove the Tuple from memory.

\section{Practical Use Case}
\label{sec:usecase}
Apache Storm is used for stream processing of data, in many sources [2,3] a word frequency count example has been shown which can be considered to be trivial, but in actual practice it can be used to execute complicated algorithms. For the purposes of this paper we consider a trivial but different example of \textbf{Sentiment Analysis}.

\subsection{Scenario}

\subsubsection{Description}
\paragraph{Let us consider} a company which produces a range of products, for example \textbf{Apple Inc.} produces IPhones, IPods, MacBooks, etc. For such a big company an understanding of the sentiment of the people is important to maintain a positive outlook. The company can also analyze the feedback people place on a specific product, to change and adjust the pricing schemes in real time to the demand.  At present there are many social media networks available and people tend to share their views on these networks,  \textbf{Twitter} is also one of these social networks where a number of posts are available publicly and can be analyzed to obtain a generalized view about a specific product or the company in general.

\subsubsection{Proposed Solution}

\paragraph{Solution} to the scenario can be implemented using \textbf{Apache Storm} to process the tweets in real time. Figure \ref{fig:solution} shows the simplified system overview, consisting of Twitter Streaming Servers as the source of information. This information is processed in the Storm Clusters from within the Company Infrastructure and the analyzed result is reported to the User Interface, which can be accessed by the appropriate user.

\begin{figure}
  \begin{center}
    \includegraphics[width=.7\textwidth]{solution.png}
    \caption{Generic Streaming Solution implemented in this use case}
    \label{fig:solution}
   \end{center}
\end{figure}

\paragraph{This} is not the only possible solution, there are multiple other tools such as \textbf{Spark Streaming}, \textbf{Samza}, etc. but this paper considers Apache Storm as it is in the spotlight of this paper, for further details regarding other available tools and their comparisons with Apache Storm, please consider the section \ref{sec:othertools} for further details.

\subsection{Practical Implementation}
The implementation of this solution concluded in three sub phases, namely Design, Development and Deployment. In the design phase, the storm topology was thoroughly planned, specifically the spouts, bolts and how they are interconnected. In the development phase, these spouts, bolts and other required aspects of the software was programmed and lastly was deployed on a locally hosted storm server.

\subsubsection{Design}
\paragraph{Designing} a storm topology consists of determining the external sources of data (spouts), the processing that is required to this data imported from external sources (bolts) and the target system. Refer to figure \ref{fig:topoimpl} for the specific topology implementation for our use case. The sources of information for the scope of this paper is only Twitter Streaming API therefore a single Spout for integrating with the Twitter API is sufficient.

To Process the stream of tweets, we need to parse these tweets injected into the topology. The ParseTweetBolt processes these tweets and then emits whether the tweet was positive or negative under the simplified scenario. These separate positive or negative remark on a tweet does not provide useful information unless it is aggregated, thus a ResultAggregationBolt is responsible for aggregating all the results of the Parser bolt and providing the result to the target in real time.

\begin{figure}
  \begin{center}
    \includegraphics[width=.7\textwidth]{topoimpl.png}
    \caption{Topology Implementation for Sentiment Analysis}
    \label{fig:topoimpl}
   \end{center}
\end{figure}

The tweet spout contacts the twitter streaming server and fetches the available tweets in real time and relays it into the topology as a single tweet per tuple. The tuple is forwarded to the TweetParseBolt which will logically analyze the tweet as a positive or a negative tweet and with the percentage probability and emits this result to the ResultAggregationBolt. The ResultAggregationBolt calculates simply the weighted mean of the positive feedback and the negative feedbacks and outputs it into a local file, which can be later observed.

\subsubsection{Development}
\paragraph{Writing} code for storm can be achieved by using different programming languages depending upon the user. Apache Storm was designed to be usable with any programming languages\footnote{\url{http://storm.apache.org/about/multi-language.html}} and currently has Adapters available for Ruby, Python, Javascript and Perl.

The only spout uses Twitter Hosebird Client (\url{https://github.com/twitter/hbc}) to connect to Twitter Streaming Server and consume the Tweets generated in real time. Whereas the Bolts were programmed in java and the code can be viewed in \url{https://github.com/hakku276/storm-sentiment-analysis}.

\section{Other Tools}
 \label{sec:othertools}
 
 <Describe shortly that other tools are available as well and list them with short descriptions>
 
 \subsection{Comparision between Storm, Spark and Samza}
 
 <Gather and show the differences between Storm, Spark and Samza in general>
 
 \subsubsection{Coding}
 <Compare how the coding is in all these platforms>
 
 \subsubsection{Tool setup}
 <Compare how easy it is to setup the tools>
 
 \subsubsection{Deployment}
 <Compare how easy it is to deploy the code onto the tool>
 
 \subsubsection{Resilience}
 <Compare the resilience model of the tools>
 
 \section{Discussion}
<What I felt about the tool, ease of use and other general aspects> 
 
 
 The zookeeper services can be run in a single machine or can be deployed in a cluster as well. A single zookeeper instance can also be shared between other services and Apache Storm, but, the major consideration to be taken into account is performance. If there are multiple number of systems that utilize the zookeeper, the Zookeeper can run out of number of clients it can service, thus affecting up-time of the systems in general.\cite{stormtwitter} Whereas having a dedicated Zookeeper allows a high performing cluster.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Bibliography}
\bibliographystyle{splncs03}
\bibliography{paper}
Alle Links wurden zuletzt am 22.06.2015 geprüft.
\end{document}
